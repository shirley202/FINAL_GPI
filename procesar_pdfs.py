import os
import re
import unicodedata
import json
import pickle
import numpy as np
import pypdf
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer

# ============================
# CONFIGURACI√ìN GENERAL
# ============================
PDF_DIR = "docs"
INDEX_DIR = "index_data"

os.makedirs(INDEX_DIR, exist_ok=True)

TFIDF_FILE = os.path.join(INDEX_DIR, "indice_tfidf.pkl")
EMB_FILE = os.path.join(INDEX_DIR, "embeddings.npy")
META_FILE = os.path.join(INDEX_DIR, "metadata.json")

EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
embedder = SentenceTransformer(EMBED_MODEL)

# ============================
# NORMALIZACI√ìN AVANZADA
# ============================
def normalizar(texto):
    """Limpieza profunda para mejorar embeddings & TF-IDF."""
    texto = texto.replace("\n", " ").replace("\r", " ")
    texto = texto.lower()
    texto = "".join(
        c for c in unicodedata.normalize("NFD", texto)
        if unicodedata.category(c) != "Mn"
    )
    texto = re.sub(r"[^a-z0-9√°√©√≠√≥√∫√±.,;:() ]", " ", texto)
    texto = re.sub(r"\s+", " ", texto).strip()
    return texto


# ============================
# EXTRAER TEXTO DE PDF
# ============================
def extraer_texto(pdf_path):
    reader = pypdf.PdfReader(pdf_path)
    texto = ""
    for p in reader.pages:
        try:
            extraido = p.extract_text()
            if extraido:
                texto += extraido + "\n"
        except:
            continue
    return texto


# ============================
# DETECTAR ESTRUCTURA NORMATIVA
# ============================
PATRON = re.compile(
    r"(cap[i√≠]tulo\s+\w+|t[i√≠]tulo\s+\w+|secci[o√≥]n\s+\w+|art[√≠i]culo\s+\d+|art\.\s*\d+)",
    re.IGNORECASE
)

def dividir_por_estructura(texto):
    texto = texto.replace("\r", "").strip()
    partes = re.split(PATRON, texto)

    chunks = []

    # estructura: [basura, "Art√≠culo 1", texto1, "Art√≠culo 2", texto2, ...]
    for i in range(1, len(partes), 2):
        etiqueta = partes[i].strip()
        contenido = partes[i+1].strip() if i+1 < len(partes) else ""

        chunks.append({
            "etiqueta": etiqueta,
            "texto": etiqueta + "\n" + contenido
        })

    return chunks


# ============================
# EXPANSI√ìN INTELIGENTE
# (Cap√≠tulo/Secci√≥n ‚Üí +contenido asociado)
# ============================
def expandir_chunks(chunks):
    """Une t√≠tulos con su contenido real si no son art√≠culos."""
    nuevos = []
    buffer = None

    for ch in chunks:
        etiqueta = ch["etiqueta"].lower()

        es_articulo = etiqueta.startswith("art")

        if not es_articulo:
            # Es un t√≠tulo ‚Üí iniciar buffer
            if buffer:
                nuevos.append(buffer)
            buffer = ch
        else:
            # Es art√≠culo ‚Üí si hay buffer, unirlo antes
            if buffer:
                nuevos.append(buffer)
                buffer = None
            nuevos.append(ch)

    if buffer:
        nuevos.append(buffer)

    return nuevos


# ============================
# RECONSTRUCCI√ìN COMPLETA
# ============================
def rebuild_index():
    print("======================================")
    print(" RECONSTRUYENDO √çNDICE H√çBRIDO FCyT")
    print("======================================")

    documentos = []

    for archivo in os.listdir(PDF_DIR):
        if not archivo.lower().endswith(".pdf"):
            continue

        ruta = os.path.join(PDF_DIR, archivo)
        print(f"\nüìÑ Procesando PDF: {archivo}")

        texto = extraer_texto(ruta)
        chunks = dividir_por_estructura(texto)
        chunks = expandir_chunks(chunks)

        for ch in chunks:
            etiqueta = ch["etiqueta"]

            articulo = etiqueta if etiqueta.lower().startswith("art") else None
            cap = etiqueta if etiqueta.lower().startswith(("cap", "sec", "t√≠t")) else None

            documentos.append({
                "fuente": archivo,
                "articulo": articulo,
                "capitulo": cap,
                "texto": ch["texto"],
                "texto_normal": normalizar(ch["texto"])
            })

    print(f"\nüîç Total de fragmentos estructurales: {len(documentos)}")

    textos_norm = [d["texto_normal"] for d in documentos]

    # ============================
    # TF-IDF
    # ============================
    print("\n‚öô Generando matriz TF-IDF...")
    vectorizer = TfidfVectorizer(max_features=25000, ngram_range=(1, 2))
    X = vectorizer.fit_transform(textos_norm)
    embeddings_tfidf = X.toarray().astype("float32")

    print("üíæ Guardando √≠ndice TF-IDF...")
    with open(TFIDF_FILE, "wb") as f:
        pickle.dump({
            "documentos": documentos,
            "vectorizer": vectorizer,
            "embeddings": embeddings_tfidf
        }, f)

    # ============================
    # EMBEDDINGS DENSOS
    # ============================
    print("\n‚öô Generando embeddings densos...")
    embeddings_dense = embedder.encode(textos_norm, convert_to_numpy=True, show_progress_bar=True)
    np.save(EMB_FILE, embeddings_dense)

    # ============================
    # METADATOS
    # ============================
    print("üíæ Guardando metadatos...")
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(documentos, f, indent=4, ensure_ascii=False)

    print("\n‚úî √çndice h√≠brido generado exitosamente.\n")


if __name__ == "__main__":
    rebuild_index()
